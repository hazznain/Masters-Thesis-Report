\chapter{Background}
\label{chapter:background} 

This chapter describes the main motivation and theoretical background behind our research. In a systematic stepwise approach we list and describe the main topics. We start with the motivation, inspiration and the partners of this thesis and then we explain the theoretical concepts with reference to previous work done on the respective topics. For each topic we also describe how it has contributed to our research. 

\section{Smart grids } \label{smartgrid}

The energy industry across the globe is facing numerous challenges. There are huge pressures from regulatory authorities and environmental organizations to reduce their carbon footprint, expand their renewable energy portfolios, and to take energy conservation measures. The demand response (DR)\footnote{Demand Response(DR); Changes in electric usage by end-use customers from their normal consumption patterns in response to changes in the price of electricity over time, or to incentive payments designed to induce lower electricity use at times of high wholesale market prices or when system reliability is jeopardised \cite{balijepalli2011review}.} and its impacts on consumer behaviour requires rapid adaptations in energy service providers' business models. According to United States Federal Energy Regulatory Commission (FERC):
 
\emph{``Demand response can provide competitive pressure to reduce wholesale power prices; increases awareness of energy usage; provides for more efficient operation of markets; mitigates market power; enhances reliability; and in combination with certain new technologies, can support the use of renewable energy resources, distributed generation, and advanced metering. Thus, enabling demand-side resources, as well as supply-side resources, improves the economic operation of electric power markets by aligning prices more closely with the value customers place on electric power''}\cite{federal2008assessment}.
  
Traditionally, power system participants have been strictly producers or consumers of electricity. The demand response and reliability issues with conventional electric power distribution models on the consumer side are causing a major trend in motivating consumers to produce electricity at a domestic level mostly using the renewable energy production methods. ``Prosumer'' is an emerging term used for an economically motivated entity i.e. \cite{grijalva2011prosumer}:
\begin{itemize}
\item Consumes, produces, and stores power,
\item Operates or owns a power grid small or large, and hence transports electricity, and
\item Optimizes the economic decisions regarding it.
\end{itemize}

The current energy grids support unidirectional distribution models and are centralized in nature.  They have very limited ability to handle the prosumer needs. Line losses and hierarchical topology makes them less reliable. They usually become bottleneck when rapid adaptations are required for the  demand response. In \cite{farhangi2010path}, Farhangi defines smart grids as:

 \emph{``The next-generation electricity grid, expected to address the major shortcomings of the existing grid. In essence, the smart grid needs to provide the utility companies with full visibility and pervasive control over their assets and services. The smart grid is required to be self-healing and resilient to system anomalies. And last but not least, a smart grids needs to empower its stakeholders to define and realize new ways of engaging with each other and performing energy transactions across the system''.}

\section{The CIVIS project} \label{civisproject}

CIVIS refers to the European Union's project for \emph{``Cities as drivers of social change''} under the seventh framework. It is part of the programme for optimising energy systems in smart cities. The CIVIS project is a collaborative effort of 10 European Universities \footnote{1.\ Associazione Trento RISE, Italy 2. Aalto university, Finland 3. Imperial College London, UK 4. ENEL Foundation, Italy 5. Instituto Superior Tecnico, Portugal 6.Karlsruhe Institute of Technology, Germany 7.Kungliga Tekniska Hogskolan, Sweden 8.SANTER REPLY SpA Italy 9.Nederlandse Organisatie voor toegepast Natuurwetenschappelijkonderzoek, Netherlands 10. Delft University of Technology,Netherlands}. It aims to embed the social aspect into the advancements of energy technology. To unleash the full potential of this vision, smart grids need to be coupled with broader social and cultural considerations and understood as complex socio-techno-economic systems with multiple decision making layers that are in effect at the physical, cyber, social, and policy making levels \cite{civisproposal}.

ICT acts as one of the main enablers for the smart grids. ICT also provides a lot of new mediums for the social aggregation e.g. internet based social media. The CIVIS project tends to connect these two different dimensions with innovative ICT solutions. An integrated approach to energy efficiency is the basic manifesto of the CIVIS project. \cite{civisproposal}

Understanding energy usage patterns and benchmarking energy efficiency performance of small units within cities are some preliminary items in the list of the CIVIS project objectives. Within the scope of our research we analyse energy data to understand the consumption patterns and evaluate various factors that can effect directly or indirectly on the usage patterns. We also try to classify the buildings on the basis of their energy efficiency and try to test the sensitivity of energy efficiency with respect to the ecological factors that can cause shift in usage patterns. For the CIVIS project aim of social aspect integration, we also present an ICT application framework that can be used to collect and analyse social media data. However the analysis of that data is not within the scope of this document. 


\section{The Green Campus initiative} \label{greencamp}

The Green Campus initiative is a project by ``Technical Research Centre of Finland'' (VTT). It is part of the EcoCampus 2030 program. The EcoCampus is an attempt to increase energy efficiency in districts and buildings by innovative management and control systems capable to optimize the local consumption without compromising the indoor environment, occupant comfort and building performance, and by introducing new ICT enabled business models \cite{ greencampus}. The vision of the program is to realize a net zero energy model for a world class research, development and educational facility. Program focuses on co-designing this model with user by educating them and then collecting feedbacks for improvement. The main aim is to improve the energy efficiency of the building units and enable them to become self sustainable for the future. As a consequence, this performance based ecosystem can help both consumers and producers to adapt with the demand response.

The Green Campus initiative is a pilot project for the EcoCampus 2030 program. VTT has installed smart devices inside Aalto University campus buildings in the cities of Espoo and Helsinki. These specialized devices are equipped with smart meters for energy consumption and indoor environment monitoring sensors.  The data used for analysis in our research was collected from a subset of buildings used as test sites for this project. The data includes hourly consumption of electricity and electricity used for heating. For one of the test sites VTT has provided us with the data of device level energy consumption details i.e. electricity used by different home appliances. This was achieved using smart NIALM-\footnote{ NIALM stands for non-intrusive appliance load monitoring,  is a process for analysing changes in the voltage and current going into a house and deducing what appliances are used in the house as well as their individual energy consumption }\cite{ hart1992nonintrusive} meters that can distinguish between different electric devices used on the basis of their signal thumb print.

Apart from providing the data, the researchers from VTT's Green Campus initiative have also helped us in formulating the use cases for this thesis research.


\section{Big Data analytics} \label{big_data_analytics}

Big Data analytics is the application of advanced data analytics techniques on large volumes of data. Advanced analytics is a generalized term used for data analysis techniques: statistical analysis, data mining, machine learning, natural language processing, text mining and data visualisation etc. \cite{russom2011big}. Although the data volume is a widely used factor for qualification of the Big Data, when it comes to Big Data analytics there are a few other important attributes i.e. variety, velocity, valuation and veracity. The concept of the 3V's (volume, variety and velocity) of data was first given by an analyst, Doug Laney from Gartner  in a 2001 MetaGroup research publication, ``3D data management: Controlling data volume, variety and velocity'' \cite{laney20013d}.  Gartner used this concept to formulate a data magnitude index that can support decision making for selection of the solutions for tackling Big Data challenges. This concept is shown in Figure~\ref{fig:3Vs}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{images/3Vs.pdf}
    \caption{Gartner 3V's of data and data magnitude index \cite{laney20013d}.}
    \label{fig:3Vs}
  \end{center}
\end{figure}

Numbers 0 to 3 represents the scale of the data that can be perceived on each dimension. Adding them together for a Big Data case can provide the data magnitude index. This method provides some basis for quantifying the data as Big Data, however it does not provide a definitive model as it allows presumptions to scale the data. Valuation and veracity are two other factors that are being used widely along with Gartner's 3V's. Valuation supports the decision making by considering the value of outcomes against the efforts required to collect, manage, process and analyse large amounts of data.  While veracity refers to ambiguity in the data that can cause complexity. There is no standard definition of Big Data but most of the attempts to define Big Data can be associated with these five factors that we have discussed.

As a matter of fact, we are not attempting to provide a definition of Big Data as part of this study or stating any criteria for qualification of a data set as Big Data. Instead we are proposing an advanced analytics model that should be capable enough to handle Big Data as well other smaller data sets. The modular architecture of the model platform can be tweaked to handle volume, variety, velocity, and veracity based on the requirements while trying to maximize the valuation for the use case. In the following subsections we discuss some of the relevant technological advancements that enable handling of the mentioned challenges of Big Data analytics. These concepts, tools and techniques are also used in developing the data analytics platform and performing the analysis for our thesis research.

\subsection{Parallel batch processing with Hadoop} \label{mapr} 
It is hard to predict the size of data and computing power required to process the data when dealing with Big Data. Scaling up  \footnote{When the need for computing power increases, a single powerful computer is added with more CPU cores, more memory, and more hard disks and used in parallel.}is an option that is always bounded by some maximum capacity limits. Also specialized hardware to scale up for higher capacity usually cost much more than general purpose hardware. So the viable option is to scale out \footnote{When the need for computing power increases, the tasks are divided between a large number of less powerful machines with (relatively) slow CPUs, moderate memory amounts, moderate hard disk counts.} using the required number of smaller machines with relatively low computing resources in parallel. From programming point of view managing parallel running processes on different machines while ensuring low failure rate, is a tough job. So the desired system should provide programmers an abstraction from lower level system details to enable rapid and fault tolerant development for Big Data applications.  MapReduce is a parallel batch processing framework developed at Google for the purpose of web indexing. The concept of MapReduce was published by Jeffrey Dean and Sanjay Ghemawat in 2008 within their research paper ``MapReduce: simplified data processing on large clusters''  \cite{dean2008mapreduce}. This paper describes MapReduce as: 

\emph{``a programming model that provides a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Programs written in this functional programming style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication''}.

Hadoop is the open source software framework whose main components are drived from MapReduce. It was developed by Doug Cutting and Mike Cafarella. It was initially created in 2005 to support an open source search engine but then adapted to the published MapReduced framework \cite{dean2008mapreduce}. It was released by the Apache foundation. Apache has also built various supporting tools around Hadoop framework to support end-to-end Big Data analytics ecosystems e.g. Apache flume for data collection, Hadoop File system (HDFS) for storing, Apache Pig and Hive for processing, Apache Mahout for machine learning.  

 Hadoop is a batch processing framework that empower processing of large volumes of data using commercial grade low cost computing infrastructure. So they support volume and valuation directly. Variety can also be supported by different file formats in HDFS. Veracity is subjected to supported tools like data collection or data mining tools. Support for such tools is available in Apache Hadoop e.g. Flume, Mahout etc. Velocity however is the only feature that a batch processing framework like Hadoop cannot handle. The next subsection answers the question of velocity.

\subsection{Real time Big Data processing }
Real time data processing is generally associated with live streams of the data. The real time data can be processed and analysed on arrival or it can be buffered for small intervals to provide near to real time analysis. However in many modern data applications instantaneous data needs to be analysed in the context of large volumes of historical data. To apply advanced analytics models such as machine learning, active feedback loops are also necessary.  Even for stored (non live data) Big Data, applications require data processing systems to answer queries very fast. To fulfil these industry-driven requirements technology is in rapid advance mode. In the last twelve to eighteen months we have seen software like YARN (Hadoop 2.0), Storm, Spark, Shark, Cloudera Impala etc. with near to real time processing capabilities. On top of it, tools like Mlbase and Cloudera Oryx have started to enable real time advance analytics. Most of these systems, frameworks and tools are being developed as the evolution path for Hadoop. All of them have their own purpose, strengths, and limitations. They are mostly used in combinations depending on the use cases. We are not discussing or comparing these systems and tools. Instead, in this article, We briefly discuss the two prevailing architectural constructs that can enable real or near to real time Big Data processing.

\subsubsection{ Lambda architecture}
Lambda architecture presents a hybrid model by using fast stream processing together with relatively slow parallel batch processing.  It was developed by Nathan Marz on the basis of knowledge and experience he gained from his work with large data sets at Twitter Inc. His approach decomposes data processing systems into three layers i.e. a batch layer, a serving layer and a speed layer. The stream of data is dispatched to both the batch and speed layers. The batch layer manages the historic data set and pre-computes the batch views. The serving layer indexes the batch views so the queries can be served with low latency as compared to traversing through the complete data set. The speed layer deals with the recent data thus compensates for the change of data sets during updates of serving layer. An answer to the query is the merged view, batch view, and the real time view \cite{marz2013big}\cite{lambdaarch}. 

Figure~\ref{fig:lambda} below shows the Lambda architecture. Lambda architecture can be implemented using combination of systems and tools e.g. Apache Hadoop along with Apache Storm.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{images/lambda.pdf}
    \caption{Lambda architecture \cite{lambdaarch}.}
    \label{fig:lambda}
  \end{center}
\end{figure}

\subsubsection{Massively parallel processing - MPP databases and query engines}\label{mpp}
MPP based architectures use multiple independent computing resources like servers, processors and storages to execute processing jobs in parallel. Most of the MPP based database approaches implement shared nothing (SN) architecture i.e. a distributed computing architecture in which each node is independent and self sufficient and there is no point of contention across the system. The SN concept for databases was first presented by Michael Stonebraker at the University of California, Berkeley in 1986 \cite{stonebraker1986case}. The SN databases have been very popular in commercial application primarily because of the high scalability offered by this architecture. Teradata Warehousing Solutions has been using SN database architectures extensively. Greenplum is an example for open an source SN database.

Despite high scalability and other positive aspects, SN databases need a lot of manual work in terms of partitioning the data, tuning the data and load balancing etc. Maintaining such database systems can be expensive. MapReduce and Apache Hadoop ecosystem provide a high level of automation, along with scalability, flexibility and fault tolerance. However parallel batch processing is not as fast SN based MPP databases. Merging of both models can solve all these issues. Cloudera Imapala is one example of the MPP based on-line query engine that runs natively on top of Hadoop \cite{ clouderaimpala}. It can provide MPP like query response time with processing power and flexibility of the Hadoop. For our research we have used Cloudera Impala for handling near to real-time velocity for Big Data processing.

\section{Energy efficiency and eco-effeciency} \label{ecoeff}
In the introductory chapter and sections \ref{smartgrid}, \ref{civisproject}, \ref{greencamp}, we have highlighted the importance of energy efficiency. We discussed the advancements in pervasive smart energy devices, grids and their role in improving energy efficiency.  We have also discussed the need for collecting and processing large volumes of data from smart energy devices and the available solutions. In this section we explain the main motivation and the theoretical concept behind data analysis.  

Unprecedented challenges arising from increasing dependency on conventional energy are part of a global phenomena. Like other economies, countries in the European Union are also putting a lot of focus on energy efficiency to ensure energy supply security  by reducing primary energy consumption and decreasing energy imports. It helps to reduce greenhouse gas emissions in a cost- effective way and thereby to mitigate climate change \cite{eu2012ee}. Member states agreed to reduce 20\% of the EU's primary energy consumption by 2020 in the council of European Union in March, 2007. The EU's Energy Efficiency Directive 2012 \cite{eu2012ee} defines energy efficiency as the ratio between output of performance, service, goods or energy, and the input of energy. This definition was first discussed in 2006 in the European Commission's action plan for energy efficiency. This generic definition covers all major aspects of the energy efficiency i.e. production, distribution, consumption and the value created in comparison to the resources consumed during the whole process.  However, to develop a methodology for measuring the energy efficiency and to evaluate the savings, the project ``Measuring and potentials of energy efficiency (EPO)'' was started in 2008 \cite{arundel2009measuring}. As part of this project VTT published a report: ``Measuring energy efficiency Indicators and potentials in buildings, communities and energy systems''\cite{forsstrommeasuring}. This report presents the model for calculating energy efficiency and its correlation with environmental factors.  VTT's research presented in this report considers energy efficiency as a subset of larger eco-efficiency. The ecological factors that can affect energy efficiency are e.g.  Temperature, CO2, NOx,SO2 etc. The ecological efficiency itself is a way of measuring sustainable development. VTT summarizes the whole ecosystem in Figure~\ref{fig:eco-effeciency} below.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{images/eco-effeciency.png}
    \caption{ Energy efficiency, eco-efficiency and sustainability\cite{forsstrommeasuring}.}
    \label{fig:eco-effeciency}
  \end{center}
\end{figure}

The concept of eco-efficiency provides the basis for data analysis in our research. We have applied basic and advanced analytics techniques on data sets collected from building units that are part of VTT's Green Campus initiative pilot project with consideration of the eco-efficiency model presented in VTT's report. We calculated energy efficiency of  the buildings on basis of formula deduced in Chapter 5 (equation 5.1 and 5.2) of VTT's  report \cite{forsstrommeasuring}.
\begin{equation}
Energy\;efficiency\;of\;a\;building  =  \frac{Energy\;consumed}{Built\;area}
\end{equation}  

In case of a specific energy consumption (SEC) \cite{forsstrommeasuring} equation 2.1 can be written as 
\begin{equation} \label{spec_energy}
SEC  = \frac{Q}{A}
\end{equation} 
Where Q denotes the consumption for a single energy type for example electricity and A is the built area in square metres. In subsequent sections we shall be  referring to these equations when we try to identify the usage patterns at building level, discuss the relevance of  energy efficiency and then discuss a model for classifying buildings by energy efficiency .

\section{Daily consumption patterns, base load and user load}\label{daily}
Daily consumption patterns of a building unit corresponds to the respective usage of the building. Understanding daily usage patterns can help in identifying the optimization points for improving the energy efficiency of that building unit. The base load of a building is one important metric that can be detected through observing the daily consumption. The base load is the consumption that takes place regardless of the actual use of the building and of the user's energy consumption  \cite{forsstrommeasuring}. It is the permanent minimum load that a power supply system is required to deliver. The base load is usually caused by the continuous consumption for building maintenance like air conditioning, ventilation, or night time lighting. Sometimes the base load also includes energy consumption by functional components inside building like computer servers, lab equipment, and refrigerators etc. However VTT differentiates the base load from the user energy load that is characterized by the direct involvement of the users of a building. For example an office building has the peak load during the day time because users are using various additional appliances like personal computers, coffee makers, lights etc. compared to the base load that is generated during the night time when the office building is not in use. Figure~\ref{fig:baseload} illustrates the concept of base load and user load.
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{images/baseload.png}
    \caption{ Base load, user load and energy efficiency \cite{forsstrommeasuring}.}
    \label{fig:baseload}
  \end{center}
\end{figure}

In the Figure~\ref{fig:baseload}, the base load is written as base consumption. The energy efficiency of base consumption and the energy efficiency of user load can be calculated using equation 2.1 or 2.2. This provides a weighted metric that can be benchmarked and compared. It can help to narrow down the scope of research by referring to problematic buildings and their issues.

\section{Energy consumption seasonal patterns}\label{seasonal}
Energy consumption has high dependency on seasonal factors like the weather. Energy consumption trends vary with outside temperature. Among other things, the energy required for the air conditioning in the buildings is a major variable factor dictating the trends. Due to regional weather differences the seasonal energy consumption patterns are also different for different regions e.g. in cold regions of the world energy consumption surges in winter while in warmer regions energy consumption increase is expected in summer. Energy service providers usually conduct demand planning with consideration of seasonal trends. Consideration of seasonal trends is also very important while optimising for gaining energy efficiency. 

In scope of our research we have also analysed the seasonal trends. It was not hard for us to perceive the trends while knowing the weather trend for localities of our test buildings. However, the interesting use case in our research was to check the sensitivity of other consumption patterns and analysis results against the seasonal trend. This will be more explained in the later part of this document where we shall discuss the results of our analysis.

Previously, there have been many studies of both daily and seasonal trends in energy consumption. Due to regional differences in trends, many of these studies focused on consumption patterns within a country. Geoffrey K.F. Tso et al.\ \\cite{tso2003study} and Yigzaw G. Yohanis et al.\ \\cite{yohanis2008real} study the energy consumption patterns in  Hong Kong and the United Kingdom respectively. Building units e.g. residential houses, apartments and commercial offices etc were considered as basic units of analysis. Yigzaw G. Yohanis's methodology resembles most to our approach as he considered ecological factor along with energy efficiency in similar way. As discussed before, the main purpose of VTT's Green Campus initiative under the EcoCampus 2030 plan is to develop a highly efficient model ecosystem for energy production, distribution and consumption that can be expanded further to any scale. Aligned to this goal, we have attempted to provide a data analysis model that is not specific to certain geographic locations. However detailed study is required for adapting such generic models to region specific requirements. In our research we have also attempted to classify the buildings on the basis of energy efficiency, which is explained in next section.

\section{Classification of buildings based on energy efficiency}\label{classify}

Earlier we mentioned that quantifiable energy efficiency through equation 2.1 and 2.2 can be used as a metric for benchmarking and comparison. For energy service providers, governmental energy regulatory agencies or research institute like VTT, it is very important to identify the problematic consumption units in group of other highly optimized or average performing consumption units. Classification of these units into similarly performing groups can help them to narrow down the focus on problematic units. Sometimes it can also help in understanding the good practices applied by certain consumption units that have improved their energy efficiency performance.
  
Classification for fault detection analysis of a building energy consumption has been used previously. Xiaoli Li et al.\ \cite{li2010classification} used classification along with the outlier detection mechanism to identify the energy inefficient building. They provide a step wise approach to extract the features (types of energy, trends etc) from the data collected as a time series. Then detect the daily usage patterns using the auto regression technique and pass the results to benchmark against any outlying data point that can refer to faulty behaviour. Imran Khan \cite{khan2013fault} proposes different clustering techniques to group the buildings with similar level of energy efficiency together. In our research we used a hybrid method using feature extraction and trend detection techniques such as \cite{li2010classification} and then applied a clustering technique proposed by Imran Khan \cite{khan2013fault}. The clustering technique that we use is called K-means clustering. It is explained in the next subsection. 

\subsection{K-means clustering}
K-means is an algorithm for cluster analysis. In the context of the machine learning, cluster analysis is an unsupervised task of grouping a set of objects in a way that objects in the same group are similar to each other more than the objects in other groups. The K-means algorithm clusters the set of objects i.e. energy efficiency values in our case into a predefined number of classes. We term these values as data points. K represents the number  of clusters or groups that we can set in start of the process. K-means means algorithm was first proposed by Stuart Llyod in 1957 \cite{lloyd1982least} but the K-means term was first used by James Mcqueen in 1967 \cite{macqueen1967some}. There have been many  adaptations and optimizations in Lloyd's basic algorithm. K-means algorithm today has many variants like Fuzzy C-means clustering, K-medoids and Spherical means etc. Even for the Lloyd's original algorithm, there has been some modification in methodology. Two very commonly used methods are the Forgy method \cite{forgy1965cluster} and the Hartigan-Wong method \cite{hartigan1979algorithm}. In our approach we are using the Hartigon-Wong method. We shall also use some references from Forgy method when explaining the K-means algorithm. 

The K-means groups the data points into clusters with logical centre points. The aim of the K-means algorithm is to divide data points within certain dimensions into K clusters so that the within-cluster sum of squares is minimized \cite{hartigan1979algorithm}. Let's assume if we want to have the K cluster for data points  D = \(\{x_1,x_2,\dotsc,x_n\}\) in \(d\) dimensions then
\begin{align*}
x_{i}& \in R ^{d}
\end{align*}
 
The K-means algorithm uses following steps to cluster data into groups \cite{ng2000cs229}.
\begin{enumerate}
  \item Initialize the centroids randomly for each K i.e. for each group. 
  \item Data points are assigned to the closest centroid.
  \item Move the centroids to the mean of the data points assigned to that centroid in step 2.
  \item Repeat 2 and 3 till convergence. Convergence means that the values have stopped changing for further iterations.
\end{enumerate} 

Mathematically randomly initialized centroids are  
$${\mu_1,\mu_2,\dotsc,\mu_k}  \in R^{n}$$


If \(c^{i}\) is the distance of centroid to assigned data point then Step 2 and 3 with recursive distance minimization and mean adjustment can be explained as 

For every i, set 
\begin{equation}
c^{i} := \arg\min_j || x^{i} - \mu_j||^2
\end{equation} 
The equation above used the Euclidean distance formula for calculating distance between centroid and data point. 

For every j, set
\begin{equation}
\mu_{j} := \frac{\sum\limits_{i=1}^n 1\{c^{i} =j\}x^{i}}{\sum\limits_{i=1}^n 1\{c^{i}=j\}}
\end{equation}

The input to the K-means is a set of feature vectors along with the number of clusters required. Before inserting data to the K-means, it is required to set the similar scale for features as well set the standard variance to avoid errors in the results. We were required to classify pilot site buildings into four groups with high efficiency, moderate efficiency, low efficiency and poor efficiency classes, So we have set K value as 4.

\section{Forecasting the energy consumption}\label{predict}
Estimating equipment-specific energy consumption has been a key focus area for energy service providers. It can help in demand planning, load forecasting, and understanding end user behaviour.  Energy service providers can design better service offerings for their consumers. Unit Energy Consumption (UEC) is a term generally used for estimating equipment specific energy consumption. It is the average annual amount of energy consumed by a user device. As part of the Green Campus project VTT has used state of the art non-intrusive load monitoring (NIALM) \cite{hart1992nonintrusive} devices that can distinguish between the usage of different electric devices on the basis of changes in voltage and electricity. 

We are using the data collected by a NIALM device installed in one of the residential apartment included in VTT's pilot test sites. We use auto regression along with the concept of the moving averages in the form of a model known as Auto-regression Integrated Moving Averages (ARIMA) to estimate the future consumption of a device depending on its previous usage. This is an example of quantitative forecasting. Before we go on to discuss about ARIMA models, it is important that we briefly discuss the basic conditions for quantitative forecasting and the time series analysis as the foundation for our prediction model based on ARIMA.
\subsection{Main conditions and Steps for Quantitative Forecasting}\label{conditions}
Rob Hyndman et al.\ \cite{hyndman2014forecasting} discuss two main conditions for application of quantitative forecasting in their book ``Forecasting: Principles and Practice'':
\begin{enumerate}
\item Numerical information about the past is available.
\item It is reasonable to assume that some aspects of the past patterns will continue into the future.
\end{enumerate}
In case the conditions can not be met then qualitative forecasting is the only option. However, qualitative forecasting is not in the scope of our research. In the same book authors mention following five step approach for solving forecasting problems.
\begin{enumerate}
\item Problem definition.
\item Information gathering that includes statistical data collection.
\item Exploratory analysis of the data to evaluate the structure of the data and observing the relationship between different variables.
\item Choosing and fitting the forecasting model. The model depends upon the relationships between variables. Every model has its own construct. So data needs to be fitted to that construct before applying that model. We discuss it more in the data analysis part of this thesis.
\item Using and evaluating the forecasting model. It generally includes comparison of results after applying different models.  
\end{enumerate}    

\subsection{Time Series Analysis}
Time series is the sequence of a random variable collected over time. Among other examples of time series data, energy consumption data from metering devices can also be collected periodically hence constituting a time series. Comparison of a single time series at different point in time is termed as time series analysis \cite{box1976time}. A time series usually consists of a deterministic component and a random component\cite{mujumdarstochastic}. So if \(X_{t}\) is a time series data then we can have 

\begin{equation}
X_{t}= d_{t}\;+\;\epsilon_{t} 
\end{equation}  

Where \(d_{t}\) is the deterministic component and \(\epsilon_{t}\) is the random component. The deterministic component itself can be in the form of trends, periods, and jumps etc. Figure ~\ref{fig:time_series} illustrates the example of different time series. In each illustration there is at least one stochastic random component with and without deterministic components. 
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{images/time_series.png}
    \caption{ Time Series types\cite{mujumdarstochastic}.}
    \label{fig:time_series}
  \end{center}
\end{figure}  

In figure ~\ref{fig:time_series} illustrations 2, 3 and 4 contain a deterministic component with a random component. When forecasting for such cases, it is possible to predict even the random component by using the deterministic component. However for stochastic random time series data without any deterministic components it is very hard to predict anything accurately. The time series with no predictable pattern is generally termed as a stationary time series. 
 
\subsection{Autoregression, Moving Averages and ARIMA} \label{ARIMA}    
 Rob Hyndman's book ``Forecasting: Principles and Practice''\cite{hyndman2014forecasting} is the main reference for this section. 
 \subsubsection{Regression}The concept behind basic regression techniques for forecasting is that we try to forecast a variable `y' on the basis of another variable `x'. For example a linear regression model forecast y assuming it has a linear relationship with  variable x e.g. as in equation below.
 $$ y = \beta_{0}\;+\;\beta_{1}x\;+\;\epsilon $$
 Parameter \(\beta_0\) and \(\beta_1\) represents the intercept and slope respectively for the line representing  the linear relationship. \(\beta_0\) represents the predicted value when x is 0. Linear regression for time series analysis can be written as 
 $$ Y_{t} = \beta_{0}\;+\;\beta_{1}x_{t-1}\;+\;\epsilon $$
 Here \(Y_{t}\) is the estimate with past value of \(x_{t}\) i.e. \(\{x_1,x_2,\dotsc, x_{t-1}\}\) . using differencing \footnote{The differences between consecutive observations} error \(e_{t}\) in estimation can be calculated as
 \begin{equation}
 e_{t}\;=\; X_{t}\; -\; Y_{t}\;=\; x_{t}\; -\;\beta_{0}\;-\;\beta_{1}x_{t-1}\;-\;\epsilon
 \end{equation}
 \subsubsection{Auto-regression}  
 The auto-regressive model is based on the concept of a variable regressing on itself. For auto-regression we can drive the equation as
  \begin{equation}
  x_{t}\; =\;\beta_{0}\;+\;\beta_{1}x_{t-1}\;+\;e_{t}\;+\;\epsilon
  \end{equation} 
  The aim for good estimation is to select values of \(\beta_0\) and \(\beta_1\) that can minimize the sum of the square of errors. The above equation can be used to estimate the value based on previous values. But in case we want to estimate based on multiple previous values e.g. `p' values then we can write it as 
 \begin{equation} \label{AR}
  x_{t}\; =\;c\;+\;\beta_{1}x_{t-1}\;+\;\beta_{2}x_{t-2}\;+\;\dotsc\;+\;\beta_{p}x_{t-p}\;+\;e_{t}\;+\;\epsilon
 \end{equation}
 
 We just replaced \(\beta_0\) with a constant c as it is a constant value. Adding the summation to the historic values we can write 
 
$$
x_{t}\; =\;c\;+\;e_{t}\;+\;\sum\limits_{i=1}^p\beta_{i}x_{t-i}
$$
we have also taken out the random component \(\epsilon\) that does  not meet the basic conditions for forecasting as described in subsection \ref{conditions}. The model presented  in equation \ref{AR} is referd to as AR\((p)\) model. 
\subsubsection{Moving Averages}
 The moving averages model uses past forecast errors in regression like manner to forecast future time series values instead of using past time series values as in auto-regression. Mathematically, the model can be explained as
 \begin{equation}\label{MA}
 y_{t}\; =\;c\;+\;e_{t}\;+\;\theta_{1}e_{t-1}\;+\;\theta_{2}e_{t-2}\;+\;\dotsc\;+\;\theta_{q}e_{t-q}
\end{equation}
OR
$$
y_{t}\; =\;c\;+\;e_{t}\;+\;\sum\limits_{i=1}^q\theta_{i}e_{t-i}
$$
The model presented  in equation \ref{MA} is termed as MA\((q)\) model. In this model each value of \(y_t\) can be thought of as a weighted moving average of the past few forecast errors.
\subsubsection{ARIMA Model}
ARIMA stands for Auto-Regressive Integrated Moving Average. As the name suggests it is the combination of the auto-regression and moving average models. ARIMA is one of the most commonly used forecasting techniques e.g. ARIMA is being used widely in stock market prediction software solutions. ARIMA model can handle time series data with and without seasonality. So combining the auto-regression and moving averages using equations \ref{AR} and \ref{MA} we can have 
\begin{equation}\label{ARIMA1}
 y^\prime_{t}\; =\;c\;+\;e_{t}\;+\;\phi_{1}y^\prime_{t-1}\;+\;\dotsc\;+\;\phi_{p}y^\prime_{t-p}\;+\;\theta_{1}e_{t-1}\;+\;\dotsc\;+\;\theta_{q}e_{t-q}
\end{equation}
In this equation \(y^\prime_t\) is the difference series. This constitutes ARIMA\((p,d,q)\) model where
\begin{itemize}
\item p is the order of the auto-regression.
\item d is the number of the non seasonal differences.
\item q is the order of the moving averages.
\end{itemize}
Now to simplify the complex time series equation back-shift notations are usually used e.g. \(y_t-1\) can be denoted by \(By_t\) i.e.
$$By_{t} = y_{t-1}$$
\&
$$B(By_{t}) = B^2y{t} = y_{t-2}  $$
\& 
$$ y_{t}\; -\; y_{t-1}\;=\; (1\;-\;B)y_{t}  $$
In general a \(d\)th order difference is written as 
$$ (1\;-\;B)^d y_{t} $$

By rearranging equation \ref{ARIMA1} and using back-shift notations, we can have the following equation with labelled p,d and q for the ARIMA model.
$$
\underbrace{1\;-\;\phi_{1}B\;-\;\dotsc\;-\;\phi_{p}B^p}_\text{AR(p)}\quad \underbrace{(1\;-\;B)^d y_{t}}_\text{d differences}\;=\;c\;+\;\underbrace{(1\;+\;\theta_{1}B\;+\;\dotsc\;+\;\theta{q}B^q)e_{t}}_\text{MA(q)}
$$

The explanation and the equations used in section \ref{ARIMA} were cherry picked from Rob Hyndman's book ``Forecasting: Principles and Practice'' \cite{hyndman2014forecasting} as reference to theory related to our research.For further details please refer to chapter 5 and chapter 8 of this book.

Fitting the ARIMA model and estimating the future time series values needs intensive computation. We use software e.g. R to solve these equations for our use cases. 




 
  






 


