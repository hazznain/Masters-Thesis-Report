\chapter{Discussion}
\label{chapter:discussion}
In this chapter we attempt to summarize our work with a critical analysis of our approach and results while highlighting some possible future directions related directly or indirectly to our research. Our research has three main constituents i.e. (i) Big Data tools and techniques (ii) Big Data analytics (iii)Using Big Data analytics to facilitate in improving energy efficiency. We focus on these topics for our discussion in this chapter.    

\section{Big Data tools and techniques}
In our approach, we tried to cover data volume, velocity, variety, veracity, and valuation as the five major aspects of Big Data. The concept and implemented platform are horizontally scalable to meet the large data set requirement. We performed most of our analysis on small size hardware described in section \ref{env}. For the proof of concept we also tested the implemented platform in a multi server environment by scaling out to four serving nodes of similar hardware specifications as explained in section \ref{env}. We used both public cloud and dedicated on-premises hard to test multi server implementation. But for the purpose of data processing during our analyses, we used single hardware server node in pseudo distributed mode i.e. using resources within a single server in distributed parallel mode. This single node configuration in pseudo distributed mode provided us enough computing power to handle the data we collected from VTT. 

The largest data set collected for our research was an approximately 250MB file with around 1.2 Million records in it. This is a small volume compared to the volumes that are generally associated with Big Data. It is important to mention the fact that this data was collected from only 40 buildings for a period of 11 months as a pilot project. The real life commercial scale use of such projects can include thousands of buildings and data processing may require including of the data recorded  during several years. To test our system's readiness for large data sets we replicated the collected data several times in the form of larger file sizes in multiple numbers. 

In terms of velocity, we analysed our data using distributed parallel batch processing. For the sake of fast and near to real-time data processing speed, we also tested Cloudera Impala. The performance of Cloudera Impala in terms of speed, was many times faster than the batch processing tools like Apache Pig and Apache Hive.  However, Cloudera Impala has very limited support for handling complex and composite data structures. There are some new emerging tools like Apache Spark and Apache Shark that can enable much faster data processing and offers support for complex data structures. Adaptation of our data platform with these new emerging tools can further enhance its data processing capabilities. 

Data variety and veracity was managed on the basis of our use case requirements and collected data sets. We built our data platform on top of Cloudera CDH that provides basic Apache Hadoop ecosystem data processing tools like Hive and Pig. On top of it we also integrated database systems like MongoDB that can ingest schema-less data to give flexibility for handling unstructured data. Statistical programming capabilities using R were also available to clean and process data. The whole platform was based on open source software and the model has the inbuilt flexibility for use in any kind of deployment models e.g. public or private cloud, own premises general purpose hardware, or combination of them. 

In an ideal data platform, the processes and workflows should be fully automated. This means that after configurations, the platform should be able to collect, process, mine, and visualise data automatically. Our data platform covered the end-to-end process for data analytics. All the components were integrated to implement the required workflows. However some manual interventions were required to execute the workflows e.g. extracted insights were fed to visualisation tools in the form of off-line CSV files. This can be improved to provide full automation using some paid services like database connectors to visualisation. Another automation can be added for operations and maintenance of the platform using tools like Opscode Chef and PuppetLabs AutomateIT tools. It is worth mentioning here that Cloudera CDH provides a management control panel to manage resources. But this control panel is limited to tools within CDH. 
\section{Big Data Analytics}
The real value of processing data lies in revelation of hidden valuable knowledge. Big Data analytics is about processing large volumes and diverse varieties of data on a need basis to discover the hidden patterns, underlying correlations and other useful information. Statistical analysis tools like R, SPSS and Matlab have a diverse range of powerful advance analytics libraries and packages that can be used to extract the information from data. But these tools allow scalability only in terms of scaling up. Such scalability is always limited and requires specialized expensive hardware. Combining the data processing power of Apache Hadoop and MapReduce with these statistical tools can enable us to process huge amounts of data and mine the required information with tools like R.

We used this concept for data processing and analysis in our reserach. We leveraged on the power of Hadoop to pre-process our bulk data and transformed it into a smaller size while keeping the useful information intact. We then applied the advance analytics using R to generate the required insights. This is a very useful approach. But in order to build a continuous data processing and mining pipeline it has its limitations e.g. manual or off-line mode for transfer of processed data between the data processing and the data mining phases. 

There is a new emerging paradigm of Big Data analytic tools that can execute variety of advance analytics techniques directly on top of the Hadoop File System. Apache Mahout is one example that can be integrated within the Apache Hadoop ecosystem and provides powerful advance analytics capabilities. Mahout uses parallel batch processing, so inherently it is limited for use in on-line streaming analytics. Cloudera Oryx and Mlbase are two new emerging tools in this ecosystem that can integrate with Apache Spark and Hadoop 2.0 to provide on-line analytics on streaming Big Data. Integration and use of these tools can improve the capabilities of platform discussed in the scope of our thesis research.     

\section{Using Big Data analytics for energy efficiency}
We used R in combination with Apache Hadoop in our data analysis for energy efficiency use cases. We described data sets in the details in section \ref{datasets}. In the same section we mentioned about the respective use cases of each data set.  For the hourly energy consumption data set we analysed the daily and seasonal usage patterns using simple aggregation methods like summation and averaging. These patterns provided the basic information about the impacts of ecological factors on consumption of some specific types of energies. We also observed the sensitivity of consumption patterns to the variations in these ecological factors. Identified seasonal trends in the data also laid the basis for our next important use case i.e. the classification of buildings on the basis of energy efficiency. 

We tried to quantify the energy efficiency of the buildings in our data set using VTT's previous research on the topic. Then we attempted to group the similarly performing (in terms of energy efficiency) buildings together and classify them into four categories ranging from highly energy efficient to highly inefficient buildings. Within our classification we also tried to observe the seasonal trends on the classification. We used the cluster analysis technique with K-means algorithm to classify the buildings. K-means algorithm requires a predefined number of clusters. In our case we had our basic requirement to categorises building into four categories. In other possible use cases, where there may not be any predefined categories, other advance analytics techniques e.g. the Hierarchical Agglomerative Clustering can be used. The use of our classification technique is to identify the possible opportunities to improve the energy efficiency and learn from good practices. As an obvious next step to our research, the respective buildings with lower energy efficiency should be explored to identify the possible energy leakages, faults and bad usage practices. Similarly the good practices can be learnt from the energy efficient buildings. Our energy efficiency classification is limited to the general purpose usage of the buildings. This classification model may raise false red flags for buildings with specialized usage like data centre buildings, laboratories and factories etc. 

For the NIALM data set with device level energy consumption information, we tried to compare different prediction models that can help energy service providers to plan for demand response, production and distribution. Our comparison was limited to the prediction model for continuously used devices like refrigerators and freezers. We predicted the future month's usage on the basis of previous monthly usage. This comparison could actually be the first step for building a prediction model for energy service providers, while a recommendation engine for users that can learn from user behaviour. The energy tariff data can be included to provide the recommendation to user to improve energy efficiency, as well as to save money from reduction from the reduction on energy spendings. This model can be improved further by adding more historic consumption data that can further affirm the seasonal and usage specific trends.    