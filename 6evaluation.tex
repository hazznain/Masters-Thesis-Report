\chapter{Discussion}
\label{chapter:discussion}
In this chapter we attempt to summarize our work with a critical analysis of our approach and results while highlighting some possible future directions related directly or indirectly to our research. Our research have three main constituents i.e. (i) Big Data tools and techniques (ii) Big Data analytics (iii)Using BIG Data analytics to facilitate in improving energy efficiency. We pivot on these topics for our discussion in this chapter.    

\section{Big Data tools and techniques}
In our approach, we tried to cover data volume, velocity, variety, veracity and valuation as the five major aspects of Big Data. The concept and implemented platform are horizontally scalable to meet the large data set requirement. We performed most of our analysis on small size hardware described in section \ref{env}. For the proof of concept we also tested the implemented platform in a multi server environment by scaling out to 4 serving nodes of similar hardware specifications as explained in section \ref{env}. We used both public cloud and dedicated on-premises hard to test multi server implementation. But for the purpose of data processing during our analysis we used single hardware server node in pseudo distributed mode i.e. using resources within a single server in distributed parallel mode. This single node configuration in pseudo distributed mode provided us enough computing power to handle the data we collected from VTT. 

The largest data set collected for our research was approximately 250MB file with around 1.2 Million records in it. This is a small volume compared to the volumes that are generally associated with Big Data. It is important to mention the fact that this data was collected from only 40 buildings for period of 11 months as a pilot project. The real life commercial scale use of such projects can include thousands of building and data processing may require to include data recorded  during several years. To test our systems readiness for large data sets we replicated the collected data several times in form of larger file sizes in multiple numbers. 

In terms of velocity, we analysed our data using distributed parallel batch processing. For sake of fast and near to real life data processing speed we also tested Cloudera Impala. The performance of Cloudera Impala in terms of speed was many times faster than the batch processing tools like Apache Pig and Apache Hive.  However Cloudera Impala has a very limited support for handling complex and composite data structures. There are some new emerging tools like Apache Spark and Apache Shark that can enable much faster data processing and offers support for complex data structures. Adaptation of our data platform with these new emerging tools can further enhance its data processing capabilities. 

Data variety and veracity was managed on the basis of our use case requirements and collected data sets. We built our data platform on top of Cloudera CDH that provides basic Apache Hadoop ecosystem data processing tools like Hive and Pig. On top of it we also integrated database system like MongoDB that can ingest schema less data too give flexibility for handling unstructured data. Statistical programming capabilities using R were also available to clean and process data. The whole platform was based on open source software and the model has the inbuilt flexibility for use in any kind of deployment models e.g. public or private cloud, own premises general purpose hardware, or combination of them. 

In an ideal data platform, The processes and work flows should be fully automated. This means that after configurations, The platform should be able to collect, process, mine and visualize data automatically. Our data platform covered the end to end process for data analytics. All the components were integrated to implement the required work-flows. However some manual interventions were required to execute the work-flows e.g. extracted insights were fed to visualization tools in form of off-line CSV files. This can be improved to provide full automation using some paid services like database connectors to visualization. Another automation can be added for operations and maintenance of platform using tools like Opscode Chef and PuppetLabs AutomateIT tools. It is worth mentioning here that Cloudera CDH provides a management control panel to manage resources. But this control panel is limited to tools within CDH. 
\section{Big Data Analytics}
The real value of processing data lies in revelation of  the hidden unknown valuable knowledge. Big Data analytics is about processing large volumes and diverse varieties of data on need basis basis to discover the hidden patterns, underlying correlations and useful information. Statistical analysis tools like R, SPSS and Matlab have diverse range of powerful advance analytics libraries and packages that can be used to extract the information from data. But these tools allow scalability only in terms of scaling up. Such scalability is always limited and requires specialized expensive hardware. Combining the data processing power of Apache Hadoop and MapReduce with these statistical tools can enable us to process huge amounts of data and the mine the required information through tools like R.

We used this concept for data processing and analysis in our reserach. We leveraged on power of Hadoop to pre-process our bulky data and transformed it into smaller size while keeping the useful information intact. We then applied the advance analytics using R to generate the required insights. This is a very useful approach. But in order to build a continuous data processing and mining pipeline it has its limitations e.g. manual or off-line mode for transfer of processed data between data processing an data mining phases. 

There is a new emerging paradigm of Big Data analytic tools that can execute variety of advance analytics techniques directly on top of Hadoop file system. Apache Mahout is one example that can integrate withing Apache Hadoop ecosystem and provides powerful advance analytics capabilities. Mahout uses parallel batch processing so inherently it is limited for use in online streaming analytics. Cloudera Oryx and Mlbase are two new emerging tools in this ecosystem that can integrate with Apache Spark and Hadoop 2.0 to provide on-line analytics on streaming Big Data. Integration and use of these tools can improve the capabilities of platform discussed in scope of our thesis research.     

\section{Using BIG Data analytics for energy efficiency}
We used R in combination with Apache Hadoop in our data analysis for energy efficiency use cases. We described data sets in details in section \ref{datasets}. In the same section we mentioned about the respective use cases of each data set.  For the hourly energy consumption data set we analysed the daily and seasonal usage patterns using simple aggregation methods like summation and averaging. These patterns provided the basic information about the impacts of ecological factors on consumption of some specific types of energies. We also observed the sensitivity of consumption patterns to the variations in these ecological factors. Identified seasonal trends in te data also laid for our next important use case i.e. classification of buildings on basis of energy efficiency. 

We tried to quantify the energy efficiency of the buildings in our data set using VTT's previous research on the topic. Then we attempted to group the similarly performing (in terms of energy efficiency) buildings together and classify them into four categories ranging from highly energy efficient to highly inefficient buildings. Within our classification we also tried to observe the seasonal trends on the classification. We used cluster analysis technique with K-means algorithm to classify the buildings. K-means algorithm requires predefined number of cluster. In our case we had our basic requirement to categorises building into four categories. In other possible use case where there are no predefined categories, other advance analytics techniques e.g. Hierarchical agglomerative clustering can be used. The use of such classification is to identify the possible opportunities to improve the energy efficiency and learn from good practices. As an obvious next step to our research, the respective buildings with lower energy efficiency should be explored to identify the possible energy leakage, faults and bad usage practices. Similarly the good practices can be learnt from the energy efficient buildings. Our energy efficiency classification is limited to the general purpose usage of the buildings. This classification model may raise false red flags for buildings with specialized usage like data centres building, laboratories and factories etc. 

For NIALM data set with device level energy consumption information, tried to compare different prediction models that can help energy service providers to plan for the demand response, production and distribution. Our comparison was limited to the prediction model for continuously used devices like refrigerators and freezers. We predicted the future one month usage on basis of previous monthly usage. This comparison can actually be the first step for building prediction model for energy service providers while a recommendation engine for users that can learn from user behaviour and the energy tariff data and provides recommendation to user to improve energy efficiency as well as reduce spending on energy. This model can be improved further by adding more historic consumption data that will have seasonal as well as user specific trends in it.    